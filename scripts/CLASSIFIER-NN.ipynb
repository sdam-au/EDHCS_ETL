{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', message='.*initial implementation of Parquet.*')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Plugin\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n"
     ]
    },
    {
     "data": {
      "text/plain": "'2.5.0'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# requires to be connected to a properly configured python environment with tensorflow etc.\n",
    "from tensorflow import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read locally\n"
     ]
    },
    {
     "data": {
      "text/plain": "         EDCS-ID                                        publication province  \\\n0  EDCS-03700724  ZPE-108-159 = Thesprotia 00001 = AE 1993, 0140...   Achaia   \n1  EDCS-03300852                                     AE 1995, 01409   Achaia   \n2  EDCS-28500283  CIL 03, 07251 = D 00214 = NDIEC-07, p 81 = AE ...   Achaia   \n3  EDCS-09400671  CIMRM-02, 02350 = IG-12, 00274 = Andros 00124 ...   Achaia   \n4  EDCS-24600769                    AE 1995, 01407 = AE 2001, 01812   Achaia   \n\n  province_list                                       place  \\\n0        Achaia                  Agios Athanasios / Photike   \n1        Achaia                                Alea / Tegea   \n2        Achaia                                Alea / Tegea   \n3        Achaia                                      Andros   \n4        Achaia  Archea Olimpia / Archaia Olympia / Olympia   \n\n                                         place_list end_yr_list  \\\n0                   ['Agios Athanasios', 'Photike']         313   \n1                                 ['Alea', 'Tegea']         276   \n2                                 ['Alea', 'Tegea']          50   \n3                                            Andros         209   \n4  ['Archea Olimpia', 'Archaia Olympia', 'Olympia']          96   \n\n                             notes_dating  \\\n0                                           \n1  to 276;   b:  276 to 282         \\n\\n    \n2                                           \n3                                           \n4                                           \n\n                                         status_list        inscr_type  ...  \\\n0  ['Augusti/Augustae', 'litterae erasae', 'ordo ...  tituli honorarii  ...   \n1           ['Augusti/Augustae', 'miliaria', 'viri']          miliaria  ...   \n2              ['Augusti/Augustae', 'leges', 'viri']             leges  ...   \n3                ['litterae erasae', 'tituli sacri']      tituli sacri  ...   \n4                                                { }               { }  ...   \n\n  within_rome nearest_city city_id_hanson city_pop_est  \\\n0       False       Dodona             31       1000.0   \n1       False        Tegea             97      46362.0   \n2       False        Tegea             97      46362.0   \n3       False       Ioulis             47       1000.0   \n4       False         Elis             35       1000.0   \n\n            city_geometry nearest_city_type nearest_city_dist  \\\n0  [20.787767, 39.546432]             minor          0.097513   \n1  [22.417226, 37.427653]               big          0.004249   \n2  [22.417226, 37.427653]               big          0.004249   \n3   [24.34625, 37.633122]             minor          0.520308   \n4  [21.435443, 37.827452]             minor          0.262624   \n\n   type_of_inscription_auto type_of_inscription_auto_prob  \\\n0     honorific inscription                           1.0   \n1         mile-/leaguestone                           1.0   \n2  public legal inscription                           1.0   \n3        votive inscription                           1.0   \n4  owner/artist inscription                           1.0   \n\n                    geometry  \n0  POINT (20.76680 39.45120)  \n1  POINT (22.41710 37.43190)  \n2  POINT (22.41710 37.43190)  \n3  POINT (24.83230 37.81880)  \n4  POINT (21.62710 37.64790)  \n\n[5 rows x 112 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>EDCS-ID</th>\n      <th>publication</th>\n      <th>province</th>\n      <th>province_list</th>\n      <th>place</th>\n      <th>place_list</th>\n      <th>end_yr_list</th>\n      <th>notes_dating</th>\n      <th>status_list</th>\n      <th>inscr_type</th>\n      <th>...</th>\n      <th>within_rome</th>\n      <th>nearest_city</th>\n      <th>city_id_hanson</th>\n      <th>city_pop_est</th>\n      <th>city_geometry</th>\n      <th>nearest_city_type</th>\n      <th>nearest_city_dist</th>\n      <th>type_of_inscription_auto</th>\n      <th>type_of_inscription_auto_prob</th>\n      <th>geometry</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>EDCS-03700724</td>\n      <td>ZPE-108-159 = Thesprotia 00001 = AE 1993, 0140...</td>\n      <td>Achaia</td>\n      <td>Achaia</td>\n      <td>Agios Athanasios / Photike</td>\n      <td>['Agios Athanasios', 'Photike']</td>\n      <td>313</td>\n      <td></td>\n      <td>['Augusti/Augustae', 'litterae erasae', 'ordo ...</td>\n      <td>tituli honorarii</td>\n      <td>...</td>\n      <td>False</td>\n      <td>Dodona</td>\n      <td>31</td>\n      <td>1000.0</td>\n      <td>[20.787767, 39.546432]</td>\n      <td>minor</td>\n      <td>0.097513</td>\n      <td>honorific inscription</td>\n      <td>1.0</td>\n      <td>POINT (20.76680 39.45120)</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>EDCS-03300852</td>\n      <td>AE 1995, 01409</td>\n      <td>Achaia</td>\n      <td>Achaia</td>\n      <td>Alea / Tegea</td>\n      <td>['Alea', 'Tegea']</td>\n      <td>276</td>\n      <td>to 276;   b:  276 to 282         \\n\\n</td>\n      <td>['Augusti/Augustae', 'miliaria', 'viri']</td>\n      <td>miliaria</td>\n      <td>...</td>\n      <td>False</td>\n      <td>Tegea</td>\n      <td>97</td>\n      <td>46362.0</td>\n      <td>[22.417226, 37.427653]</td>\n      <td>big</td>\n      <td>0.004249</td>\n      <td>mile-/leaguestone</td>\n      <td>1.0</td>\n      <td>POINT (22.41710 37.43190)</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>EDCS-28500283</td>\n      <td>CIL 03, 07251 = D 00214 = NDIEC-07, p 81 = AE ...</td>\n      <td>Achaia</td>\n      <td>Achaia</td>\n      <td>Alea / Tegea</td>\n      <td>['Alea', 'Tegea']</td>\n      <td>50</td>\n      <td></td>\n      <td>['Augusti/Augustae', 'leges', 'viri']</td>\n      <td>leges</td>\n      <td>...</td>\n      <td>False</td>\n      <td>Tegea</td>\n      <td>97</td>\n      <td>46362.0</td>\n      <td>[22.417226, 37.427653]</td>\n      <td>big</td>\n      <td>0.004249</td>\n      <td>public legal inscription</td>\n      <td>1.0</td>\n      <td>POINT (22.41710 37.43190)</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>EDCS-09400671</td>\n      <td>CIMRM-02, 02350 = IG-12, 00274 = Andros 00124 ...</td>\n      <td>Achaia</td>\n      <td>Achaia</td>\n      <td>Andros</td>\n      <td>Andros</td>\n      <td>209</td>\n      <td></td>\n      <td>['litterae erasae', 'tituli sacri']</td>\n      <td>tituli sacri</td>\n      <td>...</td>\n      <td>False</td>\n      <td>Ioulis</td>\n      <td>47</td>\n      <td>1000.0</td>\n      <td>[24.34625, 37.633122]</td>\n      <td>minor</td>\n      <td>0.520308</td>\n      <td>votive inscription</td>\n      <td>1.0</td>\n      <td>POINT (24.83230 37.81880)</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>EDCS-24600769</td>\n      <td>AE 1995, 01407 = AE 2001, 01812</td>\n      <td>Achaia</td>\n      <td>Achaia</td>\n      <td>Archea Olimpia / Archaia Olympia / Olympia</td>\n      <td>['Archea Olimpia', 'Archaia Olympia', 'Olympia']</td>\n      <td>96</td>\n      <td></td>\n      <td>{ }</td>\n      <td>{ }</td>\n      <td>...</td>\n      <td>False</td>\n      <td>Elis</td>\n      <td>35</td>\n      <td>1000.0</td>\n      <td>[21.435443, 37.827452]</td>\n      <td>minor</td>\n      <td>0.262624</td>\n      <td>owner/artist inscription</td>\n      <td>1.0</td>\n      <td>POINT (21.62710 37.64790)</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 112 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    LIREg = gpd.read_parquet(\"../data/large_data/LIREg.parquet\")\n",
    "    print(\"read locally\")\n",
    "except:\n",
    "    LIREg = gpd.read_file(\"https://zenodo.org/record/5074774/files/LIREg.geojson?download=1\", driver=\"geoJSON\")\n",
    "    LIREg.to_parquet(\"../data/large_data/LIREg.parquet\")\n",
    "    print(\"locally not available yet, need to download\")\n",
    "LIREg.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "         EDCS-ID                                        publication province  \\\n0  EDCS-03700724  ZPE-108-159 = Thesprotia 00001 = AE 1993, 0140...   Achaia   \n1  EDCS-03300852                                     AE 1995, 01409   Achaia   \n2  EDCS-28500283  CIL 03, 07251 = D 00214 = NDIEC-07, p 81 = AE ...   Achaia   \n3  EDCS-09400671  CIMRM-02, 02350 = IG-12, 00274 = Andros 00124 ...   Achaia   \n4  EDCS-24600769                    AE 1995, 01407 = AE 2001, 01812   Achaia   \n\n  province_list                                       place  \\\n0        Achaia                  Agios Athanasios / Photike   \n1        Achaia                                Alea / Tegea   \n2        Achaia                                Alea / Tegea   \n3        Achaia                                      Andros   \n4        Achaia  Archea Olimpia / Archaia Olympia / Olympia   \n\n                                         place_list end_yr_list  \\\n0                   ['Agios Athanasios', 'Photike']         313   \n1                                 ['Alea', 'Tegea']         276   \n2                                 ['Alea', 'Tegea']          50   \n3                                            Andros         209   \n4  ['Archea Olimpia', 'Archaia Olympia', 'Olympia']          96   \n\n                             notes_dating  \\\n0                                           \n1  to 276;   b:  276 to 282         \\n\\n    \n2                                           \n3                                           \n4                                           \n\n                                         status_list        inscr_type  ...  \\\n0  ['Augusti/Augustae', 'litterae erasae', 'ordo ...  tituli honorarii  ...   \n1           ['Augusti/Augustae', 'miliaria', 'viri']          miliaria  ...   \n2              ['Augusti/Augustae', 'leges', 'viri']             leges  ...   \n3                ['litterae erasae', 'tituli sacri']      tituli sacri  ...   \n4                                                { }               { }  ...   \n\n  within_rome nearest_city city_id_hanson city_pop_est  \\\n0       False       Dodona             31       1000.0   \n1       False        Tegea             97      46362.0   \n2       False        Tegea             97      46362.0   \n3       False       Ioulis             47       1000.0   \n4       False         Elis             35       1000.0   \n\n            city_geometry nearest_city_type nearest_city_dist  \\\n0  [20.787767, 39.546432]             minor          0.097513   \n1  [22.417226, 37.427653]               big          0.004249   \n2  [22.417226, 37.427653]               big          0.004249   \n3   [24.34625, 37.633122]             minor          0.520308   \n4  [21.435443, 37.827452]             minor          0.262624   \n\n   type_of_inscription_auto type_of_inscription_auto_prob  \\\n0     honorific inscription                           1.0   \n1         mile-/leaguestone                           1.0   \n2  public legal inscription                           1.0   \n3        votive inscription                           1.0   \n4  owner/artist inscription                           1.0   \n\n                    geometry  \n0  POINT (20.76680 39.45120)  \n1  POINT (22.41710 37.43190)  \n2  POINT (22.41710 37.43190)  \n3  POINT (24.83230 37.81880)  \n4  POINT (21.62710 37.64790)  \n\n[5 rows x 112 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>EDCS-ID</th>\n      <th>publication</th>\n      <th>province</th>\n      <th>province_list</th>\n      <th>place</th>\n      <th>place_list</th>\n      <th>end_yr_list</th>\n      <th>notes_dating</th>\n      <th>status_list</th>\n      <th>inscr_type</th>\n      <th>...</th>\n      <th>within_rome</th>\n      <th>nearest_city</th>\n      <th>city_id_hanson</th>\n      <th>city_pop_est</th>\n      <th>city_geometry</th>\n      <th>nearest_city_type</th>\n      <th>nearest_city_dist</th>\n      <th>type_of_inscription_auto</th>\n      <th>type_of_inscription_auto_prob</th>\n      <th>geometry</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>EDCS-03700724</td>\n      <td>ZPE-108-159 = Thesprotia 00001 = AE 1993, 0140...</td>\n      <td>Achaia</td>\n      <td>Achaia</td>\n      <td>Agios Athanasios / Photike</td>\n      <td>['Agios Athanasios', 'Photike']</td>\n      <td>313</td>\n      <td></td>\n      <td>['Augusti/Augustae', 'litterae erasae', 'ordo ...</td>\n      <td>tituli honorarii</td>\n      <td>...</td>\n      <td>False</td>\n      <td>Dodona</td>\n      <td>31</td>\n      <td>1000.0</td>\n      <td>[20.787767, 39.546432]</td>\n      <td>minor</td>\n      <td>0.097513</td>\n      <td>honorific inscription</td>\n      <td>1.0</td>\n      <td>POINT (20.76680 39.45120)</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>EDCS-03300852</td>\n      <td>AE 1995, 01409</td>\n      <td>Achaia</td>\n      <td>Achaia</td>\n      <td>Alea / Tegea</td>\n      <td>['Alea', 'Tegea']</td>\n      <td>276</td>\n      <td>to 276;   b:  276 to 282         \\n\\n</td>\n      <td>['Augusti/Augustae', 'miliaria', 'viri']</td>\n      <td>miliaria</td>\n      <td>...</td>\n      <td>False</td>\n      <td>Tegea</td>\n      <td>97</td>\n      <td>46362.0</td>\n      <td>[22.417226, 37.427653]</td>\n      <td>big</td>\n      <td>0.004249</td>\n      <td>mile-/leaguestone</td>\n      <td>1.0</td>\n      <td>POINT (22.41710 37.43190)</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>EDCS-28500283</td>\n      <td>CIL 03, 07251 = D 00214 = NDIEC-07, p 81 = AE ...</td>\n      <td>Achaia</td>\n      <td>Achaia</td>\n      <td>Alea / Tegea</td>\n      <td>['Alea', 'Tegea']</td>\n      <td>50</td>\n      <td></td>\n      <td>['Augusti/Augustae', 'leges', 'viri']</td>\n      <td>leges</td>\n      <td>...</td>\n      <td>False</td>\n      <td>Tegea</td>\n      <td>97</td>\n      <td>46362.0</td>\n      <td>[22.417226, 37.427653]</td>\n      <td>big</td>\n      <td>0.004249</td>\n      <td>public legal inscription</td>\n      <td>1.0</td>\n      <td>POINT (22.41710 37.43190)</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>EDCS-09400671</td>\n      <td>CIMRM-02, 02350 = IG-12, 00274 = Andros 00124 ...</td>\n      <td>Achaia</td>\n      <td>Achaia</td>\n      <td>Andros</td>\n      <td>Andros</td>\n      <td>209</td>\n      <td></td>\n      <td>['litterae erasae', 'tituli sacri']</td>\n      <td>tituli sacri</td>\n      <td>...</td>\n      <td>False</td>\n      <td>Ioulis</td>\n      <td>47</td>\n      <td>1000.0</td>\n      <td>[24.34625, 37.633122]</td>\n      <td>minor</td>\n      <td>0.520308</td>\n      <td>votive inscription</td>\n      <td>1.0</td>\n      <td>POINT (24.83230 37.81880)</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>EDCS-24600769</td>\n      <td>AE 1995, 01407 = AE 2001, 01812</td>\n      <td>Achaia</td>\n      <td>Achaia</td>\n      <td>Archea Olimpia / Archaia Olympia / Olympia</td>\n      <td>['Archea Olimpia', 'Archaia Olympia', 'Olympia']</td>\n      <td>96</td>\n      <td></td>\n      <td>{ }</td>\n      <td>{ }</td>\n      <td>...</td>\n      <td>False</td>\n      <td>Elis</td>\n      <td>35</td>\n      <td>1000.0</td>\n      <td>[21.435443, 37.827452]</td>\n      <td>minor</td>\n      <td>0.262624</td>\n      <td>owner/artist inscription</td>\n      <td>1.0</td>\n      <td>POINT (21.62710 37.64790)</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 112 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LIREg.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "type_of_inscription_clean\nepitaph                            21520\nvotive inscription                 11728\nowner/artist inscription            3340\nhonorific inscription               3003\nbuilding/dedicatory inscription     2561\nmile-/leaguestone                   1307\nidentification inscription           850\nacclamation                          287\ndefixio                              269\nlist                                 246\nmilitary diploma                     209\nlabel                                194\nboundary inscription                 175\nelogium                              132\nletter                               119\npublic legal inscription             109\nseat inscription                      42\nprivate legal inscription             36\nprayer                                18\nassignation inscription               15\ncalendar                              10\nadnuntiatio                            1\ndtype: int64"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EDH_overlap_all = LIREg[(LIREg[\"EDH-ID\"].notnull()) & (LIREg[\"EDCS-ID\"].notnull())]\n",
    "EDH_overlap = EDH_overlap_all[~EDH_overlap_all[\"type_of_inscription_clean\"].str.contains(\"NULL\")]\n",
    "EDH_overlap.groupby(\"type_of_inscription_clean\").size().sort_values(ascending=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "22"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(EDH_overlap.groupby(\"type_of_inscription_clean\").size().sort_values(ascending=False))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "['honorific inscription',\n 'mile-/leaguestone',\n 'public legal inscription',\n 'votive inscription',\n 'owner/artist inscription',\n 'public legal inscription',\n 'honorific inscription',\n 'building/dedicatory inscription',\n 'building/dedicatory inscription',\n 'building/dedicatory inscription']"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes_sorted = [key for key in dict(EDH_overlap.groupby(\"type_of_inscription_clean\").size().sort_values(ascending=False))]\n",
    "# alternative, treat whole dataset as a sample\n",
    "EDH_sampled = EDH_overlap\n",
    "\n",
    "y = EDH_sampled[\"type_of_inscription_clean\"].tolist()\n",
    "y[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "22"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(y))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# labels to integers (otherwise the code below does not work...)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/zg/zvg9y3rs7j527jxfq9sc2xqc0000gn/T/ipykernel_47415/1042630345.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mresults\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 7\u001B[0;31m \u001B[0mone_hot_labels\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mto_one_hot\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/var/folders/zg/zvg9y3rs7j527jxfq9sc2xqc0000gn/T/ipykernel_47415/1042630345.py\u001B[0m in \u001B[0;36mto_one_hot\u001B[0;34m(labels, dimension)\u001B[0m\n\u001B[1;32m      2\u001B[0m     \u001B[0mresults\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mzeros\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlabels\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdimension\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabel\u001B[0m \u001B[0;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlabels\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m         \u001B[0mresults\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabel\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m1.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mresults\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mIndexError\u001B[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "def to_one_hot(labels, dimension=len(set(y))):\n",
    "    results = np.zeros((len(labels), dimension))\n",
    "    for i, label in enumerate(labels):\n",
    "        results[i, label] = 1.\n",
    "    return results\n",
    "\n",
    "one_hot_labels = to_one_hot(y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import reuters\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Like with the IMDB dataset, the argument `num_words=10000` restricts the data to the 10,000 most frequently occurring words found in the \n",
    "data.\n",
    "\n",
    "We have 8,982 training examples and 2,246 test examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the IMDB reviews, each example is a list of integers (word indices):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how you can decode it back to words, in case you are curious:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_index = reuters.get_word_index()\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "# Note that our indices were offset by 3\n",
    "# because 0, 1 and 2 are reserved indices for \"padding\", \"start of sequence\", and \"unknown\".\n",
    "decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_newswire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The label associated with an example is an integer between 0 and 45: a topic index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "\n",
    "We can vectorize the data with the exact same code as in our previous example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "\n",
    "# Our vectorized training data\n",
    "x_train = vectorize_sequences(train_data)\n",
    "# Our vectorized test data\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To vectorize the labels, there are two possibilities: we could just cast the label list as an integer tensor, or we could use a \"one-hot\" \n",
    "encoding. One-hot encoding is a widely used format for categorical data, also called \"categorical encoding\". \n",
    "For a more detailed explanation of one-hot encoding, you can refer to Chapter 6, Section 1. \n",
    "In our case, one-hot encoding of our labels consists in embedding each label as an all-zero vector with a 1 in the place of the label index, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_one_hot(labels, dimension=46):\n",
    "    results = np.zeros((len(labels), dimension))\n",
    "    for i, label in enumerate(labels):\n",
    "        results[i, label] = 1.\n",
    "    return results\n",
    "\n",
    "# Our vectorized training labels\n",
    "one_hot_train_labels = to_one_hot(train_labels)\n",
    "# Our vectorized test labels\n",
    "one_hot_test_labels = to_one_hot(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there is a built-in way to do this in Keras, which you have already seen in action in our MNIST example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "one_hot_train_labels = to_categorical(train_labels)\n",
    "one_hot_test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our network\n",
    "\n",
    "\n",
    "This topic classification problem looks very similar to our previous movie review classification problem: in both cases, we are trying to \n",
    "classify short snippets of text. There is however a new constraint here: the number of output classes has gone from 2 to 46, i.e. the \n",
    "dimensionality of the output space is much larger. \n",
    "\n",
    "In a stack of `Dense` layers like what we were using, each layer can only access information present in the output of the previous layer. \n",
    "If one layer drops some information relevant to the classification problem, this information can never be recovered by later layers: each \n",
    "layer can potentially become an \"information bottleneck\". In our previous example, we were using 16-dimensional intermediate layers, but a \n",
    "16-dimensional space may be too limited to learn to separate 46 different classes: such small layers may act as information bottlenecks, \n",
    "permanently dropping relevant information.\n",
    "\n",
    "For this reason we will use larger layers. Let's go with 64 units:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "There are two other things you should note about this architecture:\n",
    "\n",
    "* We are ending the network with a `Dense` layer of size 46. This means that for each input sample, our network will output a \n",
    "46-dimensional vector. Each entry in this vector (each dimension) will encode a different output class.\n",
    "* The last layer uses a `softmax` activation. You have already seen this pattern in the MNIST example. It means that the network will \n",
    "output a _probability distribution_ over the 46 different output classes, i.e. for every input sample, the network will produce a \n",
    "46-dimensional output vector where `output[i]` is the probability that the sample belongs to class `i`. The 46 scores will sum to 1.\n",
    "\n",
    "The best loss function to use in this case is `categorical_crossentropy`. It measures the distance between two probability distributions: \n",
    "in our case, between the probability distribution output by our network, and the true distribution of the labels. By minimizing the \n",
    "distance between these two distributions, we train our network to output something as close as possible to the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating our approach\n",
    "\n",
    "Let's set apart 1,000 samples in our training data to use as a validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_val = x_train[:1000]\n",
    "partial_x_train = x_train[1000:]\n",
    "\n",
    "y_val = one_hot_train_labels[:1000]\n",
    "partial_y_train = one_hot_train_labels[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train our network for 20 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display its loss and accuracy curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()   # clear figure\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the network starts overfitting after 8 epochs. Let's train a new network from scratch for 8 epochs, then let's evaluate it on \n",
    "the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=8,\n",
    "          batch_size=512,\n",
    "          validation_data=(x_val, y_val))\n",
    "results = model.evaluate(x_test, one_hot_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Our approach reaches an accuracy of ~78%. With a balanced binary classification problem, the accuracy reached by a purely random classifier \n",
    "would be 50%, but in our case it is closer to 19%, so our results seem pretty good, at least when compared to a random baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "test_labels_copy = copy.copy(test_labels)\n",
    "np.random.shuffle(test_labels_copy)\n",
    "float(np.sum(np.array(test_labels) == np.array(test_labels_copy))) / len(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating predictions on new data\n",
    "\n",
    "We can verify that the `predict` method of our model instance returns a probability distribution over all 46 topics. Let's generate topic \n",
    "predictions for all of the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each entry in `predictions` is a vector of length 46:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficients in this vector sum to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/zg/zvg9y3rs7j527jxfq9sc2xqc0000gn/T/ipykernel_40805/398723103.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpredictions\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "np.sum(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The largest entry is the predicted class, i.e. the class with the highest probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A different way to handle the labels and the loss\n",
    "\n",
    "We mentioned earlier that another way to encode the labels would be to cast them as an integer tensor, like such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = np.array(train_labels)\n",
    "y_test = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The only thing it would change is the choice of the loss function. Our previous loss, `categorical_crossentropy`, expects the labels to \n",
    "follow a categorical encoding. With integer labels, we should use `sparse_categorical_crossentropy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new loss function is still mathematically the same as `categorical_crossentropy`; it just has a different interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On the importance of having sufficiently large intermediate layers\n",
    "\n",
    "\n",
    "We mentioned earlier that since our final outputs were 46-dimensional, we should avoid intermediate layers with much less than 46 hidden \n",
    "units. Now let's try to see what happens when we introduce an information bottleneck by having intermediate layers significantly less than \n",
    "46-dimensional, e.g. 4-dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(4, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=20,\n",
    "          batch_size=128,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Our network now seems to peak at ~71% test accuracy, a 8% absolute drop. This drop is mostly due to the fact that we are now trying to \n",
    "compress a lot of information (enough information to recover the separation hyperplanes of 46 classes) into an intermediate space that is \n",
    "too low-dimensional. The network is able to cram _most_ of the necessary information into these 8-dimensional representations, but not all \n",
    "of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further experiments\n",
    "\n",
    "* Try using larger or smaller layers: 32 units, 128 units...\n",
    "* We were using two hidden layers. Now try to use a single hidden layer, or three hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping up\n",
    "\n",
    "\n",
    "Here's what you should take away from this example:\n",
    "\n",
    "* If you are trying to classify data points between N classes, your network should end with a `Dense` layer of size N.\n",
    "* In a single-label, multi-class classification problem, your network should end with a `softmax` activation, so that it will output a \n",
    "probability distribution over the N output classes.\n",
    "* _Categorical crossentropy_ is almost always the loss function you should use for such problems. It minimizes the distance between the \n",
    "probability distributions output by the network, and the true distribution of the targets.\n",
    "* There are two ways to handle labels in multi-class classification:\n",
    "    ** Encoding the labels via \"categorical encoding\" (also known as \"one-hot encoding\") and using `categorical_crossentropy` as your loss \n",
    "function.\n",
    "    ** Encoding the labels as integers and using the `sparse_categorical_crossentropy` loss function.\n",
    "* If you need to classify data into a large number of categories, then you should avoid creating information bottlenecks in your network by having \n",
    "intermediate layers that are too small."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "tensorflow",
   "language": "python",
   "display_name": "Python 3.9 (tensorflow)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}